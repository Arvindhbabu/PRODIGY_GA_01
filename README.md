Text Generation with GPT-2 | Prodigy Infotech Internship

## Project Overview
This project demonstrates fine-tuning of OpenAI's GPT-2 model on a custom dataset for text generation. The goal is to train a model that can generate coherent, contextually relevant, and stylistically rich text based on a given prompt.

## Objective
To fine-tune GPT-2 using a poetic + AI-themed dataset and generate creative responses that reflect the tone and style of the training data.

##  Dataset Used
- File: `custom_data.txt`
- Type: Plain text
- Style: Poetic reflections on AI, technology, and machine consciousness
- Size: ~200 lines (repeated content for training)

##  Tech Stack
- Python
- HuggingFace Transformers
- PyTorch
- Google Colab

##  Installation
Install the required packages:
```bash
pip install transformers datasets torch
